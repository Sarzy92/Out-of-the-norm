{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b777f7af",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c9d907d",
   "metadata": {},
   "source": [
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "<meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n",
    "<style>\n",
    "* {\n",
    "  margin: 0;\n",
    "  padding: 0;\n",
    "}\n",
    ".bg-img {\n",
    "/* The image used */\n",
    "  background-image: url(\"climateChang.png\");\n",
    "  min-height: 550px;\n",
    "  /* Center and scale the image nicely */\n",
    "  background-position: center;\n",
    "  background-repeat: no-repeat;\n",
    "  background-size: cover;\n",
    "  /* Needed to position the navbar */\n",
    "  position: relative;\n",
    "}\n",
    "/* Position the navbar container inside the image */\n",
    ".container {\n",
    "  position: absolute;\n",
    "  margin: 0px;\n",
    "  width: auto;\n",
    "}\n",
    ".img-ed{\n",
    "  left: -40px;\n",
    "  top:10px;\n",
    "  color: green;\n",
    "}\n",
    "nav{\n",
    "  background-color: transparent;\n",
    "  height: 60px;\n",
    "  width: auto;\n",
    "}\n",
    "ul {\n",
    "  background-color: transparent;\n",
    "  list-style-type: none;\n",
    "  Margin-left: 20px;\n",
    "}\n",
    "li {\n",
    "  background-color: transparent;\n",
    "}\n",
    "nav>ul {\n",
    "  height: 30px;\n",
    "  line-height: 30px;\n",
    "  width: auto;\n",
    "}\n",
    "nav>ul>li {\n",
    "  display: inline-block;\n",
    "  position: relative;\n",
    "  border-radius: 5px;\n",
    "  padding: 7px 15px;\n",
    "  margin-left: 10px;\n",
    "}\n",
    "nav ul li a {\n",
    "  font-size: 13px;\n",
    "  font-weight: bold;\n",
    "  text-decoration: none;\n",
    "  display: block;\n",
    "  color: white;\n",
    "  padding: 5px 5px;\n",
    "}\n",
    "a:hover{\n",
    "  text-decoration: none;\n",
    "  color: white;\n",
    "  background-color: #C4B454;\n",
    "  padding: 5px 5px;\n",
    "}\n",
    "nav>ul>li:hover>ul{\n",
    "  display: block;\n",
    "}\n",
    "nav>ul>li>ul {\n",
    "  position: absolute;\n",
    "  top: 45px;\n",
    "  left: 16px;\n",
    "  margin-left: -1px;\n",
    "  display: none;\n",
    "  background-color: grey;\n",
    "  min-width: 260px;\n",
    "  box-shadow: 0px 15px 16px 0px rgba(0,0,0,0.2);\n",
    "  z-index: 1;\n",
    "}\n",
    "nav>ul>li>ul>li {\n",
    "  position: relative;\n",
    "}\n",
    "nav>ul>li>ul>li:hover>ul {\n",
    "  display: block;\n",
    "}\n",
    "nav>ul>li>ul>li>ul {\n",
    "  position: absolute;\n",
    "  left: 16px;\n",
    "  background-color: grey;\n",
    "  margin-left: 110px;\n",
    "  top: 40px;\n",
    "  display: none;\n",
    "  width: 100%;\n",
    "  z-index: 1;\n",
    "}\n",
    ".climate-AR15 {\n",
    "  text-align: center;\n",
    "  margin-top: 150px;\n",
    "  color: black;\n",
    "  font-weight: bold;\n",
    "  background-color: #fff;\n",
    "  background-position: cover;\n",
    "  background-size: inherit;\n",
    "  opacity: 0.65;\n",
    "}\n",
    ".climate-AR15 h1 b {\n",
    "  color: red;\n",
    "  animation-name: example;\n",
    "  animation-duration: 4s;\n",
    "}\n",
    "@keyframes example {\n",
    "  from {color: red;}\n",
    "  to {color: yellow;}\n",
    "}\n",
    ".climate-AR15 p{\n",
    "  color: black;\n",
    "}\n",
    ".climate-AR15 p b {\n",
    "  color: red;\n",
    "  animation-name: example;\n",
    "  animation-duration: 4s;\n",
    "}\n",
    "@keyframes example {\n",
    "  from {color: red;}\n",
    "  to {color: yellow;}\n",
    "}\n",
    ".clim-img{\n",
    "  margin-top: 161px;\n",
    "  margin-left:2px;\n",
    "}\n",
    "</style>\n",
    "</head>\n",
    "<body>\n",
    "<div class=\"bg-img\" id=\"content\">\n",
    "  <div class=\"container\">\n",
    "    <nav>\n",
    "      <ul>\n",
    "        <li class=\"img-ed\"><a href=\"https://explore-datascience.net/\"><img src=\"street-view-solid.svg\" alt=\"Gem\" width=\"25\" height=\"25\"></a></li>\n",
    "        <li><a href=\"#introduction\">INTRODUCTION</a>\n",
    "          <ul>\n",
    "            <li><a href=\"#problem_statement\">Problem Statement</a></li>\n",
    "            <li><a href=\"#importing_libraries\">Importing Libraries</a></li>\n",
    "            <li><a href=\"#loading_data\">Loading Data</a></li>\n",
    "            <li><a href=\"#data\">Data Description</a></li>\n",
    "          </ul>\n",
    "        </li>\n",
    "        <li><a href=\"#preprocessing\">DATA PRE-PROCESSING</a>\n",
    "          <ul>\n",
    "            <li><a href=\"#null_entries\">Checking For Null Entries</a></li>\n",
    "            <li><a href=\"#empty_strings\">Checking For Empty Strings</a></li>\n",
    "            <li><a href=\"#user_handles\">Masking User Handles</a></li>\n",
    "            <li><a href=\"#urls\">Replacing URLs</a></li>\n",
    "            <li><a href=\"#punctuations\">Delete Numbers/Special Characters</a></li>\n",
    "            <li><a href=\"#stopwords\">Removing Stopwords/.Lower()/Lemmatizing</a></li>\n",
    "          </ul>\n",
    "        </li>\n",
    "        <li><a href=\"#EDA\">EXPLORATORY DATA ANALYSIS</a>\n",
    "          <ul>\n",
    "            <li><a href=\"#wordcloud\">Creating WordCloud</a></li>\n",
    "            <li><a href=\"#data_distribution\">Data Distribution</a></li>\n",
    "          </ul>\n",
    "        </li>\n",
    "        <li><a href=\"#model\">MODELLING</a>\n",
    "          <ul>\n",
    "            <li><a href=\"#imbalancedata\">Handling Imbalance Datasets</a>\n",
    "              <ul>\n",
    "                <li><a href=\"#withoutresample\">Without Resampling</a></li>\n",
    "                <li><a href=\"#upsample\">Resampling with Upsampling Technique</a></li>\n",
    "                <li><a href=\"#downsample\">Resampling with Downsampling Technique</a></li>\n",
    "                <li><a href=\"#smote\">Resampling with SMOTE Technique</a></li>\n",
    "              </ul>\n",
    "            </li>\n",
    "            <li><a href=\"#modeltraining\">Model Training</a>\n",
    "              <ul>\n",
    "                <li><a href=\"#logisticregression\">Logistic Regression</a></li>\n",
    "                <li><a href=\"#naivebayes\">Naive Bayes</a></li>\n",
    "                <li><a href=\"#linearsvc\">Support Vector Machine Models</a></li>\n",
    "                <li><a href=\"#xgb\">XGBOOST</a></li>\n",
    "              </ul>\n",
    "            </li>\n",
    "          <li><a href=\"#evaluatemodel\">Model Evaluation</a></li>\n",
    "          </ul>\n",
    "        </li>\n",
    "        <li><a href=\"#conclusion_and_recommendation\">CONCLUSION</a>\n",
    "          <ul>\n",
    "            <li><a href=\"#conclusion)\">Conclusion</a></li>\n",
    "            <li><a href=\"#recommendation\">Recommendation</a></li>\n",
    "          </ul>\n",
    "        </li>\n",
    "        <li><a href=\"#ref\">REFERENCES</a></li>    \n",
    "      </ul>\n",
    "    </nav>\n",
    "    <div class=\"climate-AR15\">\n",
    "        <h1><b>CLIMATE</b> CHANGE BELIEF ANALYSIS <b>2022</b></h1>\n",
    "        <p>An individual’s belief in <b>climate change</b> based on historical tweet data</p>\n",
    "    </div>\n",
    "    <div class=\"clim-img\">\n",
    "        <img src=\"edsa.jpg\" alt=\"EDSA\" width=\"105\" height=\"105\">    \n",
    "    </div>\n",
    "  </div>\n",
    "</div>\n",
    "</body>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505f164e",
   "metadata": {},
   "source": [
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "<meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n",
    "<style>\n",
    "body {\n",
    "  margin: 0;\n",
    "  font-family: Arial, Helvetica, sans-serif;\n",
    "}\n",
    "#introduction {\n",
    "  overflow: hidden;\n",
    "  background-color: #333;\n",
    "  display: flex;\n",
    "}\n",
    "#introduction a {\n",
    "  float: left;\n",
    "  color: #f2f2f2;\n",
    "  text-align: center;\n",
    "  padding: 14px 16px;\n",
    "  text-decoration: none;\n",
    "  font-size: 17px;\n",
    "}\n",
    "#introduction a:hover {\n",
    "  background-color: #ddd;\n",
    "  color: green;\n",
    "}\n",
    "#introduction a.active {\n",
    "  background-color: #04AA6D;\n",
    "  color: white;\n",
    "}\n",
    ".apc {\n",
    "  margin-top:10px;\n",
    "}\n",
    "</style>\n",
    "</head>\n",
    "<body>\n",
    "    <div class=\"APC\">\n",
    "        <div id=\"introduction\">\n",
    "          <a href=\"#content\"><h1>INTRODUCTION</h1></a>\n",
    "          <a href=\"#problem_statement\" class=\"apc\"><img src=\"arrow-down-long-solid.svg\" alt=\"arrow-down\" width=\"35\" height=\"35\"></a>\n",
    "        </div>\n",
    "        <div class=\"LP\">\n",
    "            <p><br>A shift in a region's regular weather patterns is referred to as climate change. The Earth's climate has grown in temperature during the previous few decades. Local climates all throughout the world are being affected by this trend.\n",
    "            Weather changes are unavoidable. However, weather and climate are not synonymous. The change in temperature and precipitation in a location from day to day is referred to as weather. By glancing outside, you can describe the weather in your neighborhood. Today's weather is if it's snowing right now. Climate, on the other hand, refers to a location's average weather across time. The weather has the potential to change abruptly.<br><br>\n",
    "            <b>Why is the climate changing ? </b><br><br> These changes to Earth’s climate are not natural shifts. Scientists are confident that human activities are leading to climate change. Human activities release gases that change the makeup of Earth’s atmosphere. These gases are making our atmosphere better at trapping the Sun's heat. We call this the greenhouse effect.. The greenhouse effect is the main cause of rising temperatures. So what is the greenhouse effect? Plants can grow better in a greenhouse because it stays warmer than the outside air. This is because heat from the Sun is able to enter the clear glass or plastic. The heat warms the air inside. The heat from the trapped air keeps the greenhouse warm.</p>\n",
    "        </div>        \n",
    "    </div>\n",
    "</body>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4496700c",
   "metadata": {},
   "source": [
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "<meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n",
    "<style>\n",
    "body {\n",
    "  margin: 0;\n",
    "  font-family: Arial, Helvetica, sans-serif;\n",
    "}\n",
    "#problem_statement {\n",
    "  display: flex;\n",
    "}\n",
    "#problem_statement a {\n",
    "  float: left;\n",
    "  color: #f2f2f2;\n",
    "  text-align: center;\n",
    "  padding: 10px 12px;\n",
    "  text-decoration: none;\n",
    "  font-size: 11px;\n",
    "}\n",
    "#problem_statement a:hover {\n",
    "  background-color: #ddd;\n",
    "  color: green;\n",
    "}\n",
    "#problem_statement a.active {\n",
    "  background-color: #04AA6D;\n",
    "  color: white;\n",
    "}\n",
    ".apc {\n",
    "  margin-top:10px;\n",
    "}\n",
    "</style>\n",
    "</head>\n",
    "<body>\n",
    "    <div class=\"APC\">\n",
    "        <div id=\"problem_statement\">\n",
    "          <a href=\"#content\"><h1>1.1 Problem Statement</h1></a>\n",
    "          <a href=\"#importing_libraries\" class=\"apc\"><img src=\"arrow-down-long-solid.svg\" alt=\"arrow-down\" width=\"15\" height=\"15\"></a>\n",
    "        </div>\n",
    "        <div class=\"LP\">\n",
    "          <p><br>Many companies are built around lessening one’s environmental impact or carbon footprint. They offer products and services that are environmentally friendly and sustainable, in line with their values and ideals. They would like to determine how people perceive climate change and whether or not they believe it is a real threat. This would add to their market research efforts in gauging how their product/service may be received. With this context, EDSA is challenging you during the Classification Sprint with the task of creating a Machine Learning model that is able to classify whether or not a person believes in climate change, based on their novel tweet data. Providing an accurate and robust solution to this task gives companies access to a broad base of consumer sentiment, spanning multiple demographic and geographic categories - thus increasing their insights and informing future marketing strategies.<br>Many companies are built around lessening one’s environmental impact or carbon footprint. They offer products and services that are environmentally friendly and sustainable, in line with their values and ideals. They would like to determine how people perceive climate change and whether or not they believe it is a real threat. This would add to their market research efforts in gauging how their product/service may be received.<br>With this context, EDSA is challenging you during the Classification Sprint with the task of creating a Machine Learning model that is able to classify whether or not a person believes in climate change, based on their novel tweet data.<br>Providing an accurate and robust solution to this task gives companies access to a broad base of consumer sentiment, spanning multiple demographic and geographic categories - thus increasing their insights and informing future marketing strategies.</p>\n",
    "        </div>      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9319af06",
   "metadata": {},
   "source": [
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "<meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n",
    "<style>\n",
    "body {\n",
    "  margin: 0;\n",
    "  font-family: Arial, Helvetica, sans-serif;\n",
    "}\n",
    "#importing_libraries {\n",
    "  display: flex;\n",
    "}\n",
    "#importing_libraries a {\n",
    "  float: left;\n",
    "  color: #f2f2f2;\n",
    "  text-align: center;\n",
    "  padding: 10px 12px;\n",
    "  text-decoration: none;\n",
    "  font-size: 11px;\n",
    "}\n",
    "#importing_libraries a:hover {\n",
    "  background-color: #ddd;\n",
    "  color: green;\n",
    "}\n",
    "#importing_libraries a.active {\n",
    "  background-color: #04AA6D;\n",
    "  color: white;\n",
    "}\n",
    ".apc {\n",
    "  margin-top:10px;\n",
    "}\n",
    "</style>\n",
    "</head>\n",
    "<body>\n",
    "    <div class=\"APC\">\n",
    "        <div id=\"loading_data\">\n",
    "          <a href=\"#content\"><h1>1.2 Importing Libraries</h1></a>\n",
    "          <a href=\"#loading_data\" class=\"apc\"><img src=\"arrow-down-long-solid.svg\" alt=\"arrow-down\" width=\"15\" height=\"15\"></a>\n",
    "        </div>\n",
    "    </div>\n",
    " </body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461d1d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Natural Language Toolkit\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize, TreebankWordTokenizer\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import seaborn as sns\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "sns.set()\n",
    "\n",
    "#Model packages\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.over_sampling import SMOTE, SVMSMOTE\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.utils import resample\n",
    "\n",
    "#Metrics/Evaluation\n",
    "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b3724e",
   "metadata": {},
   "source": [
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "<meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n",
    "<style>\n",
    "body {\n",
    "  margin: 0;\n",
    "  font-family: Arial, Helvetica, sans-serif;\n",
    "}\n",
    "#importing_libraries {\n",
    "  display: flex;\n",
    "}\n",
    "#importing_libraries a {\n",
    "  float: left;\n",
    "  color: #f2f2f2;\n",
    "  text-align: center;\n",
    "  padding: 10px 12px;\n",
    "  text-decoration: none;\n",
    "  font-size: 11px;\n",
    "}\n",
    "#importing_libraries a:hover {\n",
    "  background-color: #ddd;\n",
    "  color: green;\n",
    "}\n",
    "#importing_libraries a.active {\n",
    "  background-color: #04AA6D;\n",
    "  color: white;\n",
    "}\n",
    ".apc {\n",
    "  margin-top:10px;\n",
    "}\n",
    "</style>\n",
    "</head>\n",
    "<body>\n",
    "    <div class=\"APC\">\n",
    "      <div id=\"importing_libraries\">\n",
    "        <a href=\"#content\"><h1>1.2 Importing Libraries</h1></a>\n",
    "        <a href=\"#loading_data\" class=\"apc\"><img src=\"arrow-down-long-solid.svg\" alt=\"arrow-down\" width=\"15\" height=\"15\"></a>\n",
    "      </div>\n",
    "    </div>\n",
    "    <div><br>\n",
    "      <p>Data The collection of this data was funded by a Canada Foundation for Innovation JELF Grant to Chris Bauch, University of Waterloo. The dataset aggregates tweets pertaining to climate change collected between Apr 27, 2015 and Feb 21, 2018. In total, 43943 tweets were collected. Each tweet is labelled as one of the following classes:<br></p>\n",
    "      <ul>Class Description\n",
    "        <li>2 News: the tweet links to factual news about climate change</li>\n",
    "        <li>1 Pro: the tweet supports the belief of man-made climate change</li>\n",
    "        <li>0 Neutral: the tweet neither supports nor refutes the belief of man-made climate change</li>\n",
    "        <li>-1 Anti: the tweet does not believe in man-made climate change</li>\n",
    "      </ul><br>\n",
    "      <ul>Variable definitions\n",
    "        <li>sentiment: Sentiment of tweet</li>\n",
    "        <li> message: Tweet body</li>\n",
    "        <li>tweetid: Twitter unique id</li>\n",
    "      </ul>\n",
    "\n",
    "    </div>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f65fb04",
   "metadata": {},
   "source": [
    "Data The collection of this data was funded by a Canada Foundation for Innovation JELF Grant to Chris Bauch, University of Waterloo. The dataset aggregates tweets pertaining to climate change collected between Apr 27, 2015 and Feb 21, 2018. In total, 43943 tweets were collected. Each tweet is labelled as one of the following classes:\n",
    "\n",
    "Class Description\n",
    "* 2 News: the tweet links to factual news about climate change\n",
    "* 1 Pro: the tweet supports the belief of man-made climate change\n",
    "* 0 Neutral: the tweet neither supports nor refutes the belief of man-made climate change\n",
    "* -1 Anti: the tweet does not believe in man-made climate change\n",
    "\n",
    "Variable definitions\n",
    "- sentiment: Sentiment of tweet\n",
    "- message: Tweet body\n",
    "- tweetid: Twitter unique id\n",
    "\n",
    "Files available for download\n",
    "* train.csv - You will use this data to train your model.\n",
    "* test.csv - You will use this data to test your model.\n",
    "* SampleSubmission.csv - is an example of what your submission file should look like. The order of the rows does not matter, but the names of the tweetid's must be correct."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81eded20",
   "metadata": {},
   "source": [
    "### 1.4 Loading Data<a id=\"loading_data\"></a>\n",
    "[Table of Contents](#contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4012192b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test_with_no_labels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d6b843",
   "metadata": {},
   "source": [
    "The dataframes will be copied so as to make sure the initial dataframes do not change. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bba646",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train2 = df_train.copy()\n",
    "df_test2 = df_test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cb5e7e",
   "metadata": {},
   "source": [
    "Let’s check the first few rows of the both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1b667f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08ce189",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d8e711",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing<a id=\"preprocessing\"></a>\n",
    "[Table of Contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858efd83",
   "metadata": {},
   "source": [
    "Initial data cleaning requirements that we can think of after looking at the top 5 records:\n",
    "\n",
    "- The Twitter handles are not masked. We will maske them as *@user*.\n",
    "\n",
    "- All URLs will be replaced with the word *url*.\n",
    "\n",
    "- We also get rid of the punctuations, numbers and special characters as they add no value to the machine learning process.\n",
    "\n",
    "- Most of the smaller words do not add much value. For example, ‘of’, ‘his’, ‘all’. These are called *stopwords*. We will remove them from our data.\n",
    "\n",
    "- Once we have executed the above steps, we can split every Tweet into individual words. i.e tokens, which is an essential step in any NLP task.\n",
    "\n",
    "- In the 5th tweet of the train dataset, there is a word ‘racist’. We might also have terms like racism, race, racial, racialist etc. in the rest of the data. These terms are often used in the same context. If we can reduce them to their root word, which is ‘race’, then we can reduce the total number of unique words in our data without losing a significant amount of information. This process is known as *lemmatization*.\n",
    "\n",
    "- Converting all text to one case. Lowercase will be used here.\n",
    "\n",
    "But first, let us start by doing the most basic data cleaning processes which include;\n",
    "- Checking if there are any nulls in the datasets\n",
    "- Checking if any strings in the datasets are empty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32d51b3",
   "metadata": {},
   "source": [
    "### 2.1 Checking for Null Entries <a id=\"null_entries\"></a>\n",
    "[Table of Contents](#contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84bbaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking nulls in the train set\n",
    "df_train2.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36c2933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking nulls in the test set\n",
    "df_test2.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cab5d4b",
   "metadata": {},
   "source": [
    "Both datasets have no *Null* values, therefore, no action needs to be taken at this stage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c1e5ee",
   "metadata": {},
   "source": [
    "### 2.2 Checking for Empty Strings<a id=\"empty_strings\"></a>\n",
    "[Table of Contents](#contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448ecb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for empty strings in the train dataframe\n",
    "# start with an empty list\n",
    "empty_str = [] \n",
    "\n",
    "# iterate over the dataframe\n",
    "for i,sentiment,message,tweetid in df_train2.itertuples():  \n",
    "    # only loop through string values\n",
    "    if type(message)==str:            \n",
    "        # check for whitespace\n",
    "        if message.isspace():         \n",
    "            # if true, append index numbers to empty_str list\n",
    "            empty_str.append(i)     \n",
    "        \n",
    "print(len(empty_str), 'Empty String(s): ', empty_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edde72e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for empty strings in the test dataframe\n",
    "# start with an empty list\n",
    "empty_str = [] \n",
    "\n",
    "# iterate over the dataframe\n",
    "for i,message,tweetid in df_test2.itertuples():  \n",
    "    # only loop through string values\n",
    "    if type(message)==str: \n",
    "        # check for whitespace\n",
    "        if message.isspace():\n",
    "            # if true, append index numbers to empty_str list\n",
    "            empty_str.append(i)     \n",
    "        \n",
    "print(len(empty_str), 'Empty String(s): ', empty_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3060fedc",
   "metadata": {},
   "source": [
    "Both datasets have no empty strings, therefore, no action needs to be taken at this stage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3be12b",
   "metadata": {},
   "source": [
    "### 2.3 Masking User Handles<a id=\"user_handles\"></a>\n",
    "[Table of Contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106df11f",
   "metadata": {},
   "source": [
    "We are masking the Twitter handles with @user because;\n",
    "- of privacy concerns. \n",
    "- they hardly give any information about the nature of the tweet/sentiments\n",
    "- having different handle names will add to the total number of unique words in our data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194cc0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# masking handles names to @user on train dataset\n",
    "# define the regular pattern to recognize user handles \n",
    "regex_pattern = r'\\B@\\w+'\n",
    "# define the substitute text that will be used to replace handle name\n",
    "sub_text = r'@user'\n",
    "\n",
    "# replace user handle with substitute text on train dataset\n",
    "df_train2['message'] = df_train2['message'].replace(to_replace = regex_pattern, value = sub_text, regex = True)\n",
    "\n",
    "\n",
    "# replace user handle with substitute text on test dataset\n",
    "df_test2['message'] = df_test2['message'].replace(to_replace = regex_pattern, value = sub_text, regex = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced2bd0e",
   "metadata": {},
   "source": [
    "Let us check if the masking was successful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83c558a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9973f4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4c36be",
   "metadata": {},
   "source": [
    "We have successfully masked the user handles for both the train and test dataframes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177b234a",
   "metadata": {},
   "source": [
    "### 2.4 Replacing URLs<a id=\"urls\"></a>\n",
    "[Table of Contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e7fb44",
   "metadata": {},
   "source": [
    "URLs will be replaced with the word *url*;\n",
    "- so as to reduce the number of uniue values\n",
    "- because they add no value to our machine learning process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d1f139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing URLs with the word url on train and test datasets\n",
    "# define the regular pattern to recognize the URLs\n",
    "regex_pattern = r'http[s]?://(?:[A-Za-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9A-Fa-f][0-9A-Fa-f]))+'\n",
    "# define the substitute text that will be used to replace the URLs\n",
    "sub_text = r'url'\n",
    "\n",
    "# replace URLs with substitute text on train datatset\n",
    "df_train2['message'] = df_train2['message'] .replace(to_replace = regex_pattern, value = sub_text, regex = True)\n",
    "\n",
    "# replace URLs with substitute text on test datatset\n",
    "df_test2['message'] = df_test2['message'] .replace(to_replace = regex_pattern, value = sub_text, regex = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf12cab",
   "metadata": {},
   "source": [
    "### 2.5 Delete Punctuations, Numbers and Special Characters<a id=\"punctuations\"></a>\n",
    "[Table of Contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501a1664",
   "metadata": {},
   "source": [
    "This step is necessary because punctuations, numbers and special characters don't help in differentiating different kinds of tweets. If we skip this step, there is a higher chance that we will be working with noisy and inconsistent data. We will be replacing everything except characters and hashtags with spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df586a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove special characters, numbers, punctuations on train and test datasets\n",
    "# define the regular pattern \n",
    "regex_pattern = r'[^a-zA-Z#]'\n",
    "# define the substitute text as a whitespace\n",
    "sub_text = r' '\n",
    "\n",
    "# replace the characters on train dataframe\n",
    "df_train2['message'] = df_train2['message'] .replace(to_replace = regex_pattern, value = sub_text, regex = True)\n",
    "\n",
    "# replace the characters on test dataframe\n",
    "df_test2['message'] = df_test2['message'] .replace(to_replace = regex_pattern, value = sub_text, regex = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c96a92",
   "metadata": {},
   "source": [
    "### 2.6 Removing Stopwords, Converting Words to Lowercase, and Lemmatizing<a id=\"stopwords\"></a>\n",
    "[Table of Contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7f9765",
   "metadata": {},
   "source": [
    "Stopwords do not contribute much to the machine learning model, so it's good to remove them. A list of stopwords have been defined by the Natural Language Toolkit (nltk) library so we will start by downloading them.\n",
    "\n",
    "Lemmatization is defined as \"doing things properly using vocabulary and morphological analysis of words\"<sup>1</sup>. This process returns the base form of a word, i.e, the *lemma*.\n",
    "Example: Better - Good. For this step,  we will download WordNet from nltk. WordNet is a lexical database of Englishwhich helps find conceptual relationships between words.<sup>2</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccef105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# downloading stopwrods from nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033ba3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize set of stopwords from English dictionary\n",
    "stop_words = set(stopwords.words(\"english\")) \n",
    "# initialize a lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# define function to clean the tweets\n",
    "def clean_tweet(text):\n",
    "    # convert the text to lowercase\n",
    "    text = text.lower()\n",
    "    # lemmatization + tokenization process\n",
    "    text = [lemmatizer.lemmatize(token) for token in text.split(\" \")]\n",
    "    text = [lemmatizer.lemmatize(token, \"v\") for token in text]\n",
    "    # removing stopwords\n",
    "    text = [word for word in text if not word in stop_words]\n",
    "    text = \" \".join(text)\n",
    "    return text\n",
    "\n",
    "# cleaning train dataset\n",
    "df_train2['cleaned_message'] = df_train2.message.apply(lambda x: clean_tweet(x))\n",
    "\n",
    "# cleaning test dataset\n",
    "df_test2['cleaned_message'] = df_test2.message.apply(lambda x: clean_tweet(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366b10dc",
   "metadata": {},
   "source": [
    "Let us have a look at what the clean data looks like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2df1705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the preprossed train dataframe \n",
    "df_train2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1418514c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the preprossed test dataframe \n",
    "df_test2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8cca72",
   "metadata": {},
   "source": [
    "The preprocessed dataframes look good! We will not move to Exploratory Data Analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1effa125",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis<a id=\"EDA\"></a>\n",
    "[Table of Contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37d5ded",
   "metadata": {},
   "source": [
    "In this step, we will start by checking the News, Pro, Neutral and Anti messages frequency distribution in the train dataframe.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d08d67c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# initialize news, pro, neutral and anti messages\n",
    "news = df_train2[df_train2['sentiment'] == 2]['cleaned_message']\n",
    "pro = df_train2[df_train2['sentiment'] == 1]['cleaned_message']\n",
    "neutral =df_train2[df_train2['sentiment'] == 0]['cleaned_message']\n",
    "anti = df_train2[df_train2['sentiment'] ==-1]['cleaned_message']\n",
    "\n",
    "print('Number of news tagged sentences is:     {}'.format(len(news)))\n",
    "print('Number of pro tagged sentences is:      {}'.format(len(pro)))\n",
    "print('Number of neutral tagged sentences is:  {}'.format(len(neutral)))\n",
    "print('Number of anti tagged sentences is:     {}'.format(len(anti)))\n",
    "print('Total length of the data is:            {}'.format(df_train2.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2b7909",
   "metadata": {},
   "source": [
    "Here is a graphical representation of the same;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d2fa3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train2['sentiment'].value_counts().plot(kind ='bar')\n",
    "#labels = df_train2['sentiment'].unique()\n",
    "#label_title = ['pro', 'news', 'neutral', 'anti']\n",
    "plt.title ('The frequency of sentiment')\n",
    "#plt.xticks(labels)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fd9b15",
   "metadata": {},
   "source": [
    "Considering the size of the dataset, the count of sentenses do not seem to be equally distributed between **news, pro, neutral** and **anti**.  **Pro** has the most number of Tweets followed by **news** then **neutral** and finally **anti**. This means that majority of the tweets in this data set support the belief that climate-change is man-made while a majority of the Tweets are opposed to this. \n",
    "\n",
    "This uneven distribution of sentiments will affect the accuracy of our model(s). To solve this imbalance, resampling method  (Downscalling and Upscalling), which will involve modifying the number of observations in each class, will be used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a96c5d2",
   "metadata": {},
   "source": [
    "### 3.1 Creating WordCloud<a id=\"wordcloud\"></a>\n",
    "[Table of Contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb830ee7",
   "metadata": {},
   "source": [
    "We can now analyse the data to get further understanding of it by plotting word clouds for news, pro, netral and anti tweet messages from our train dataset and see which words occur the most.\n",
    "\n",
    "A word cloud is a collection of words depicted in different sizes. The bigger and bolder the word appears, the more often it's mentioned within a dataset. We will used the WordCloud library for this scenario. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9501213d",
   "metadata": {},
   "source": [
    "From this graphical representation, we can see that the two palceholder values we used during data preporocessing, *@user* and *@url* have weight on the word count yet they do not add any value to our machine learning process. We will therefore delete them on both train and test dataframes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d5b975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace @user and url with whitespace on train dataframe\n",
    "df_train2['cleaned_message'] = df_train2['cleaned_message'] .replace(to_replace = '@user', value = ' ', regex = True)\n",
    "df_train2['cleaned_message'] = df_train2['cleaned_message'] .replace(to_replace = 'url', value = ' ', regex = True)\n",
    "\n",
    "# replace @user and urlwith whitespace on test dataframe\n",
    "df_test2['cleaned_message'] = df_test2['cleaned_message'] .replace(to_replace = '@user', value = ' ', regex = True)\n",
    "df_test2['cleaned_message'] = df_test2['cleaned_message'] .replace(to_replace = 'url', value = ' ', regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832f8b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate wordcloud\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from PIL import Image\n",
    "\n",
    "# initialize news, pro, neutral and anti messages\n",
    "news = df_train2[df_train2['sentiment'] == 2]['cleaned_message']\n",
    "pro = df_train2[df_train2['sentiment'] == 1]['cleaned_message']\n",
    "neutral =df_train2[df_train2['sentiment'] == 0]['cleaned_message']\n",
    "anti = df_train2[df_train2['sentiment'] ==-1]['cleaned_message']\n",
    "\n",
    "news = [word for line in news for word in line.split()]\n",
    "pro = [word for line in pro for word in line.split()]\n",
    "neutral = [word for line in neutral for word in line.split()]\n",
    "anti= [word for line in anti for word in line.split()]\n",
    "\n",
    "# Define a function to plot word cloud\n",
    "def plot_cloud(news, pro, neutral, anti):\n",
    "    # Import image to np.array\n",
    "    #mask = np.array(Image.open('comment.png'))\n",
    "    mask1 = np.array(Image.open('twitter1.png'))\n",
    "    \n",
    "    news = WordCloud(random_state=1, background_color='white', colormap='Set2', \n",
    "                      collocations=False, stopwords = STOPWORDS, mask=mask1).generate(' '.join(news))\n",
    "    \n",
    "    pro = WordCloud(random_state=1, background_color='white', colormap='Set2', \n",
    "                      collocations=False, stopwords = STOPWORDS, mask=mask1).generate(' '.join(pro))\n",
    "    \n",
    "    neutral = WordCloud(random_state=1, background_color='white', colormap='Set2', \n",
    "                      collocations=False, stopwords = STOPWORDS, mask=mask1).generate(' '.join(neutral))\n",
    "    \n",
    "    anti = WordCloud(random_state=1, background_color='white', colormap='Set2', \n",
    "                      collocations=False, stopwords = STOPWORDS, mask=mask1).generate(' '.join(anti))\n",
    "    # Set figure size\n",
    "    plt.figure(figsize = (20,16))\n",
    "    plt.subplot(2,2,1)\n",
    "    plt.title('Words from news tweets', fontsize = 20)\n",
    "    # Display image\n",
    "    plt.imshow(news) \n",
    "    # No axis details\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    plt.subplot(2,2,2)\n",
    "    plt.title('Words from pro tweets', fontsize = 20)\n",
    "    # Display image\n",
    "    plt.imshow(pro) \n",
    "    # No axis details\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    \n",
    "    plt.subplot(2,2,3)\n",
    "    plt.title('Words from neutral tweets', fontsize = 20)\n",
    "    # Display image\n",
    "    plt.imshow(neutral) \n",
    "    # No axis details\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "\n",
    "    plt.subplot(2,2,4)\n",
    "    plt.title('Words from anti tweets', fontsize = 20)\n",
    "    # Display image\n",
    "    plt.imshow(anti) \n",
    "    # No axis details\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "\n",
    "plot_cloud(news, pro, neutral, anti)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee13b329",
   "metadata": {},
   "source": [
    "## 4. Modeling<a id=\"model\"></a>\n",
    "[Table of Contents](#contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5043b23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define split function\n",
    "def split_train_test(df, feature, target):\n",
    "    X = df[feature]\n",
    "    y = df[target]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 20)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329867db",
   "metadata": {},
   "source": [
    "### 4.1 Handling Imbalance Datasets<a id=\"imbalancedata\"></a>\n",
    "[Table of Contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de466bc2",
   "metadata": {},
   "source": [
    "#### 4.1.1 Without Resampling<a id=\"withoutresample\"></a>\n",
    "[Table of Contents](#contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21c7c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = split_train_test(df_train2, 'cleaned_message', 'sentiment')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea65e304",
   "metadata": {},
   "source": [
    "#### 4.1.2 Resampling with Upsampling Technique<a id=\"upsample\"></a>\n",
    "[Table of Contents](#contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacfea4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the sentiment classes\n",
    "news = df_train2[df_train2['sentiment'] == 2]\n",
    "pro = df_train2[df_train2['sentiment'] == 1]\n",
    "neutral =df_train2[df_train2['sentiment'] == 0]\n",
    "anti = df_train2[df_train2['sentiment'] ==-1]\n",
    "\n",
    "# upsample the minority classes\n",
    "news_upsample = resample(news, \n",
    "                           replace=True,\n",
    "                           n_samples=len(pro), \n",
    "                           random_state=42)\n",
    "\n",
    "neutral_upsample = resample(neutral, \n",
    "                           replace=True,\n",
    "                           n_samples=len(pro), \n",
    "                           random_state=42)\n",
    "\n",
    "anti_upsample = resample(anti, \n",
    "                           replace=True,\n",
    "                           n_samples=len(pro), \n",
    "                           random_state=42)\n",
    "\n",
    "# concatenate the upsampled dataframe\n",
    "df_train2_upsample = pd.concat([news_upsample, pro, neutral_upsample ,anti_upsample])\n",
    "df_train2_upsample['sentiment'].value_counts().plot(kind ='bar')\n",
    "plt.title ('The frequency of sentiment')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#Split upsample into train and test set\n",
    "X_train_upsample, X_test_upsample, y_train_upsample, y_test_upsample = split_train_test(df_train2_upsample, 'cleaned_message', \n",
    "                                                                                        'sentiment')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63a52a3",
   "metadata": {},
   "source": [
    "#### 4.1.3 Resampling with Downsampling Technique<a id=\"downsample\"></a>\n",
    "[Table of Contents](#contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be09b07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the sentiment classes\n",
    "news = df_train2[df_train2['sentiment'] == 2]\n",
    "pro = df_train2[df_train2['sentiment'] == 1]\n",
    "neutral =df_train2[df_train2['sentiment'] == 0]\n",
    "anti = df_train2[df_train2['sentiment'] ==-1]\n",
    "\n",
    "# downsample the minority classes\n",
    "\n",
    "news_downsample = resample(news, \n",
    "                           replace=True,\n",
    "                           n_samples=len(anti), \n",
    "                           random_state=42)\n",
    "\n",
    "pro_downsample = resample(pro, \n",
    "                           replace=True,\n",
    "                           n_samples=len(anti), \n",
    "                           random_state=42)\n",
    "\n",
    "neutral_downsample = resample(neutral, \n",
    "                           replace=True,\n",
    "                           n_samples=len(anti), \n",
    "                           random_state=42)\n",
    "\n",
    "\n",
    "# concatenate the downsampled dataframe\n",
    "df_train2_downsample = pd.concat([news_downsample, pro_downsample, neutral_downsample ,anti])\n",
    "df_train2_downsample['sentiment'].value_counts().plot(kind ='bar')\n",
    "plt.title ('The frequency of sentiment')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "X_train_downsample, X_test_downsample, y_train_downsample, y_test_downsample = split_train_test(df_train2_downsample, \n",
    "                                                                                                'cleaned_message', 'sentiment')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35c5f68",
   "metadata": {},
   "source": [
    "#### 4.1.4 Resampling with SMOTE Technique<a id=\"smote\"></a>\n",
    "[Table of Contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e95c3b",
   "metadata": {},
   "source": [
    "SMOTE stands for Synthetic Minority Oversampling Technique. This is a statistical technique for increasing the number of cases in your dataset in a balanced way. The module works by generating new instances from existing minority cases that you supply as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3564b848",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train2['cleaned_message']\n",
    "y = df_train2['sentiment']\n",
    "vect = CountVectorizer(max_df=0.90, min_df=2, ngram_range=(1,2), stop_words='english')\n",
    "\n",
    "# bag-of-words feature matrix\n",
    "X_vect = vect.fit_transform(X)\n",
    "\n",
    "# Resampling the minority class. The strategy can be changed as required.\n",
    "sm = SMOTE(random_state = 2) \n",
    "svmsmote = SVMSMOTE(random_state = 101)\n",
    "\n",
    "# Fit the model to generate the data.\n",
    "X_smote, y_smote = svmsmote.fit_resample(X_vect, y)\n",
    "\n",
    "#Split into train and test set\n",
    "X_train_smote, X_test_smote, y_train_smote, y_test_smote = train_test_split(X_smote, y_smote,test_size = 0.2, random_state=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba65015",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create train model function\n",
    "def train_model(model, X_train, X_test, y_train, y_test):\n",
    "    pipe = Pipeline([('vect', CountVectorizer(stop_words='english', \n",
    "                             min_df=1, \n",
    "                             max_df=0.9, \n",
    "                             ngram_range=(1, 3))),('tfidf', TfidfTransformer()),('model', model)])\n",
    "    pipe.fit(X_train, y_train)\n",
    "    y_pred = pipe.predict(X_test)\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e67949e",
   "metadata": {},
   "source": [
    "### 4.2 Model Training<a id=\"modeltraining\"></a>\n",
    "[Table of Contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a7f9a5",
   "metadata": {},
   "source": [
    "#### 4.2.1 Logistic Regression<a id=\"logisticregression\"></a>\n",
    "[Table of Contents](#contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154bec62",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426b99e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Without Resampling\n",
    "train_model(model, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dea1f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# With upsampling\n",
    "train_model(model, X_train_upsample, X_test_upsample, y_train_upsample, y_test_upsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549816ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With downsampling\n",
    "train_model(model, X_train_downsample, X_test_downsample, y_train_downsample, y_test_downsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fac32c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#With SMOTE\n",
    "model.fit(X_train_smote, y_train_smote)\n",
    "y_pred = model.predict(X_test_smote)\n",
    "print(classification_report(y_test_smote, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136db5d6",
   "metadata": {},
   "source": [
    "#### 4.2.2 Naive Bayes<a id=\"naivebayes\"></a>\n",
    "[Table of Contents](#contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc311f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a32ebf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# without resampling\n",
    "train_model(model, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2451a9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With upsampling\n",
    "train_model(model, X_train_upsample, X_test_upsample, y_train_upsample, y_test_upsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03ec2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With downsampling\n",
    "train_model(model, X_train_downsample, X_test_downsample, y_train_downsample, y_test_downsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b844ed4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#With SMOTE\n",
    "model.fit(X_train_smote, y_train_smote)\n",
    "y_pred = model.predict(X_test_smote)\n",
    "print(classification_report(y_test_smote, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924be370",
   "metadata": {},
   "source": [
    "#### 4.2.3 Support Vector Machine Models<a id=\"linearsvc\"></a>\n",
    "[Table of Contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccd855a",
   "metadata": {},
   "source": [
    "Support Vector Machines are classified as a classification method, but they can be used to solve both classification and regression problems. It can handle both continuous and categorical variables with ease. To differentiate various classes, SVM creates a hyperplane in multidimensional space. It iteratively generates the best hyperplane, which is then utilized to minimize an error. The goal of SVM is to find a maximum marginal hyperplane (MMH) that splits a dataset into classes as evenly as possible<sup>3</sup>.\n",
    "\n",
    "In this session, we shall apply the Support Vector Machine models as we deploy the various kernel available in the SVM model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1476697d",
   "metadata": {},
   "source": [
    "##### SVM Kernels\n",
    "\n",
    "A kernel is used to implement the SVM algorithm in practice. An input data space is transformed into the appropriate form using a kernel. The kernel trick is a technique used by SVM. The kernel transforms a low-dimensional input space into a higher-dimensional space in this case. To put it another way, it turns nonseparable issues into separable problems by adding more dimensions to them. It is most beneficial in problems with non-linear separation. The kernel trick aids in the development of a more accurate classifier.\n",
    "\n",
    "We shall apply the following kernels:\n",
    "- Linear Kernel\n",
    "- Polynomial Kernel\n",
    "- Radial Basis Function Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48529624",
   "metadata": {},
   "source": [
    "#### LinearSVC Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d60e1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = svm.SVC(kernel='linear') # Linear Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4691b8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# without resampling\n",
    "train_model(model, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babb750f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With upsampling\n",
    "train_model(model, X_train_upsample, X_test_upsample, y_train_upsample, y_test_upsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c1e270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With downsampling\n",
    "train_model(model, X_train_downsample, X_test_downsample, y_train_downsample, y_test_downsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12635f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#With SMOTE\n",
    "model.fit(X_train_smote, y_train_smote)\n",
    "y_pred = model.predict(X_test_smote)\n",
    "print(classification_report(y_test_smote, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c87790",
   "metadata": {},
   "source": [
    "#### PolynomialSVC Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f358b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = svm.SVC(kernel='poly') # Polynomial Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd31f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# without resampling\n",
    "train_model(model, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3010a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With upsampling\n",
    "train_model(model, X_train_upsample, X_test_upsample, y_train_upsample, y_test_upsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641657ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With downsampling\n",
    "train_model(model, X_train_downsample, X_test_downsample, y_train_downsample, y_test_downsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbc3790",
   "metadata": {},
   "outputs": [],
   "source": [
    "#With SMOTE\n",
    "model.fit(X_train_smote, y_train_smote)\n",
    "y_pred = model.predict(X_test_smote)\n",
    "print(classification_report(y_test_smote, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7deeb26",
   "metadata": {},
   "source": [
    "#### Radial Basis Function Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f71fe61",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_svm = svm.SVC(kernel='rbf') # Radial Basis Function Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef9521a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# without resampling\n",
    "train_model(model, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dfda26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# With upsampling\n",
    "train_model(model, X_train_upsample, X_test_upsample, y_train_upsample, y_test_upsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ead14b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With downsampling\n",
    "train_model(model, X_train_downsample, X_test_downsample, y_train_downsample, y_test_downsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4471773",
   "metadata": {},
   "outputs": [],
   "source": [
    "#With SMOTE\n",
    "model.fit(X_train_smote, y_train_smote)\n",
    "y_pred = model.predict(X_test_smote)\n",
    "print(classification_report(y_test_smote, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a87c68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a7176c5",
   "metadata": {},
   "source": [
    "## 5. Conclusion and Recommendation<a id=\"conclusion_and_recommendation\"></a>\n",
    "[Table of Contents](#contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c35595d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469c09c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d018e0f",
   "metadata": {},
   "source": [
    "### 5.1 Conclusion<a id=\"conclusion\"></a>\n",
    "[Table of Contents](#contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24396a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384ba1fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "abdf8b0d",
   "metadata": {},
   "source": [
    "### 5.2 Recommendation<a id=\"recommendation\"></a>\n",
    "[Table of Contents](#contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5825d984",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab40e5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef6f5bd8",
   "metadata": {},
   "source": [
    "## 6. References<a id=\"references\"></a>\n",
    "[Table of Contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1d9ec2",
   "metadata": {},
   "source": [
    "1. Stemming and lemmatization - Stanford NLP Group: https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html\n",
    "2. Source code for nltk.corpus.reader.wordnet: https://www.nltk.org/_modules/nltk/corpus/reader/wordnet.html\n",
    "3. Support Vector Machines with Scikit-learn Tutorial: https://www.datacamp.com/tutorial/svm-classification-scikit-learn-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4303203e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.1 32-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "0ea4eeddcb7492287e6486c97d2d09b7335b3c9edfb49b66231fc49009993d6c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
